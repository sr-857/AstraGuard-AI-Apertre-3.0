#!/usr/bin/env python3
"""
Microbenchmark Runner for AstraGuard AI

Runs pytest-benchmark tests and exports results for comparison against baselines.

Usage:
    python tools/benchmarks/run_microbench.py
    python tools/benchmarks/run_microbench.py --pattern "bench_fallback*"
    python tools/benchmarks/run_microbench.py --compare baselines/initial.json
"""

import argparse
import json
import subprocess
import sys
from datetime import datetime
from pathlib import Path

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def run_benchmarks(pattern: str | None, output_file: Path, compare_file: Path | None) -> int:
    """Run pytest-benchmark and export results."""
    
    benchmarks_dir = PROJECT_ROOT / "benchmarks"
    results_dir = benchmarks_dir / "results"
    results_dir.mkdir(exist_ok=True)

    # Build pytest command
    cmd = [
        sys.executable, "-m", "pytest",
        str(benchmarks_dir),
        "--benchmark-only",
        "--benchmark-json", str(output_file),
        "--benchmark-warmup=on",
        "--benchmark-min-rounds=5",
        "-v",
    ]

    if pattern:
        cmd.extend(["-k", pattern])

    if compare_file and compare_file.exists():
        cmd.extend(["--benchmark-compare", str(compare_file)])

    print(f"Running: {' '.join(cmd)}")
    print("=" * 60)

    result = subprocess.run(cmd, cwd=str(PROJECT_ROOT))

    if result.returncode == 0:
        print("=" * 60)
        print(f"Benchmark results saved to: {output_file}")
    
    return result.returncode


def compare_with_baseline(results_file: Path, baseline_file: Path, threshold_pct: float = 10.0) -> bool:
    """Compare benchmark results against baseline, detecting regressions."""
    
    if not results_file.exists():
        print(f"ERROR: Results file not found: {results_file}")
        return False
    
    if not baseline_file.exists():
        print(f"WARNING: Baseline file not found: {baseline_file}")
        print("Run with --save-baseline to create initial baseline.")
        return True

    with open(results_file, "r", encoding="utf-8") as f:
        results = json.load(f)

    with open(baseline_file, "r", encoding="utf-8") as f:
        baseline = json.load(f)

    # Extract benchmark stats
    results_benchmarks = {b["name"]: b for b in results.get("benchmarks", [])}
    baseline_benchmarks = {b["name"]: b for b in baseline.get("benchmarks", [])}

    regressions = []
    improvements = []
    
    print("\n" + "=" * 60)
    print("BASELINE COMPARISON")
    print("=" * 60)
    print(f"{'Benchmark':<50} {'Change':>12} {'Status':>10}")
    print("-" * 72)

    for name, result in results_benchmarks.items():
        if name not in baseline_benchmarks:
            print(f"{name:<50} {'N/A':>12} {'NEW':>10}")
            continue

        baseline_median = baseline_benchmarks[name]["stats"]["median"]
        result_median = result["stats"]["median"]

        if baseline_median == 0:
            continue

        change_pct = ((result_median - baseline_median) / baseline_median) * 100

        if change_pct > threshold_pct:
            status = "REGRESSION"
            regressions.append((name, change_pct))
        elif change_pct < -threshold_pct:
            status = "IMPROVED"
            improvements.append((name, change_pct))
        else:
            status = "OK"

        print(f"{name:<50} {change_pct:>+11.1f}% {status:>10}")

    print("-" * 72)
    
    if improvements:
        print(f"\nðŸŽ‰ {len(improvements)} benchmark(s) improved by more than {threshold_pct}%")
    
    if regressions:
        print(f"\nâš ï¸  {len(regressions)} benchmark(s) regressed by more than {threshold_pct}%:")
        for name, pct in regressions:
            print(f"   - {name}: +{pct:.1f}%")
        return False
    
    print("\nâœ“ No significant regressions detected.")
    return True


def save_as_baseline(results_file: Path, baseline_file: Path) -> None:
    """Copy results file as the new baseline."""
    if not results_file.exists():
        print(f"ERROR: Results file not found: {results_file}")
        return

    with open(results_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    # Add metadata
    data["baseline_created"] = datetime.now().isoformat()
    data["baseline_note"] = "Generated by run_microbench.py --save-baseline"

    baseline_file.parent.mkdir(parents=True, exist_ok=True)
    with open(baseline_file, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)

    print(f"Baseline saved to: {baseline_file}")


def main():
    parser = argparse.ArgumentParser(
        description="Run AstraGuard AI microbenchmarks",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Run all benchmarks
    python tools/benchmarks/run_microbench.py

    # Run specific benchmarks
    python tools/benchmarks/run_microbench.py --pattern "bench_fallback*"

    # Compare against baseline
    python tools/benchmarks/run_microbench.py --compare benchmarks/baselines/initial.json

    # Save current results as baseline
    python tools/benchmarks/run_microbench.py --save-baseline
        """
    )

    parser.add_argument(
        "--pattern", "-k",
        default=None,
        help="pytest -k pattern to filter benchmarks"
    )
    parser.add_argument(
        "--output", "-o",
        default=None,
        help="Output JSON file (default: benchmarks/results/run_TIMESTAMP.json)"
    )
    parser.add_argument(
        "--compare", "-c",
        default=None,
        help="Baseline file to compare against"
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=10.0,
        help="Regression threshold percentage (default: 10%%)"
    )
    parser.add_argument(
        "--save-baseline",
        action="store_true",
        help="Save results as the new baseline"
    )

    args = parser.parse_args()

    # Setup paths
    results_dir = PROJECT_ROOT / "benchmarks" / "results"
    results_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if args.output:
        output_file = Path(args.output)
    else:
        output_file = results_dir / f"run_{timestamp}.json"

    baseline_file = PROJECT_ROOT / "benchmarks" / "baselines" / "initial.json"

    # Run benchmarks
    exit_code = run_benchmarks(
        args.pattern,
        output_file,
        Path(args.compare) if args.compare else None
    )

    if exit_code != 0:
        print(f"\nBenchmarks failed with exit code {exit_code}")
        sys.exit(exit_code)

    # Compare against baseline if requested
    if args.compare:
        compare_file = Path(args.compare)
        success = compare_with_baseline(output_file, compare_file, args.threshold)
        if not success:
            sys.exit(1)
    elif baseline_file.exists():
        # Auto-compare against default baseline
        compare_with_baseline(output_file, baseline_file, args.threshold)

    # Save as baseline if requested
    if args.save_baseline:
        save_as_baseline(output_file, baseline_file)


if __name__ == "__main__":
    main()
